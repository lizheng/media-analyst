"""
MediaCrawler Streamlit ç•Œé¢
æä¾›ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ Web ç•Œé¢æ¥é…ç½®å’Œè¿è¡Œ MediaCrawler
"""

import streamlit as st
import subprocess
import os
from pathlib import Path

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="MediaCrawler æ§åˆ¶å°",
    page_icon="ğŸ•·ï¸",
    layout="wide",
)

# MediaCrawler è·¯å¾„
MEDIA_CRAWLER_PATH = Path("../MediaCrawler")

# å¹³å°é…ç½®
PLATFORMS = {
    "xhs": "å°çº¢ä¹¦",
    "dy": "æŠ–éŸ³",
    "ks": "å¿«æ‰‹",
    "bili": "Bç«™",
    "wb": "å¾®åš",
    "tieba": "è´´å§",
    "zhihu": "çŸ¥ä¹",
}

# ç™»å½•æ–¹å¼
LOGIN_TYPES = {
    "qrcode": "æ‰«ç ç™»å½•",
    "phone": "æ‰‹æœºå·ç™»å½•",
    "cookie": "Cookie ç™»å½•",
}

# çˆ¬è™«ç±»å‹
CRAWLER_TYPES = {
    "search": "æœç´¢æ¨¡å¼",
    "detail": "è¯¦æƒ…æ¨¡å¼",
    "creator": "åˆ›ä½œè€…æ¨¡å¼",
}

# ä¿å­˜æ ¼å¼
SAVE_OPTIONS = ["json", "csv", "excel", "sqlite", "db", "mongodb", "postgres"]


def run_crawler(args: list) -> subprocess.Popen:
    """è¿è¡Œ MediaCrawler"""
    cmd = ["uv", "run", "main.py"] + args
    return subprocess.Popen(
        cmd,
        cwd=MEDIA_CRAWLER_PATH,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )


def build_args(
    platform: str,
    login_type: str,
    crawler_type: str,
    keywords: str = "",
    specified_ids: str = "",
    creator_ids: str = "",
    start_page: int = 1,
    get_comment: bool = False,
    get_sub_comment: bool = False,
    headless: bool = True,
    save_option: str = "json",
    max_comments: int = 100,
    save_path: str = "",
) -> list:
    """æ„å»ºå‘½ä»¤è¡Œå‚æ•°"""
    args = [
        "--platform", platform,
        "--lt", login_type,
        "--type", crawler_type,
        "--start", str(start_page),
        "--save_data_option", save_option,
        "--max_comments_count_singlenotes", str(max_comments),
    ]

    if crawler_type == "search" and keywords:
        args.extend(["--keywords", keywords])
    elif crawler_type == "detail" and specified_ids:
        args.extend(["--specified_id", specified_ids])
    elif crawler_type == "creator" and creator_ids:
        args.extend(["--creator_id", creator_ids])

    if get_comment:
        args.append("--get_comment")
        args.append("yes")
    else:
        args.append("--get_comment")
        args.append("no")

    if get_sub_comment:
        args.append("--get_sub_comment")
        args.append("yes")
    else:
        args.append("--get_sub_comment")
        args.append("no")

    if headless:
        args.append("--headless")
        args.append("yes")
    else:
        args.append("--headless")
        args.append("no")

    if save_path:
        args.extend(["--save_data_path", save_path])

    return args


# é¡µé¢æ ‡é¢˜
st.title("ğŸ•·ï¸ MediaCrawler æ§åˆ¶å°")
st.markdown("é€šè¿‡ Web ç•Œé¢é…ç½®å’Œè¿è¡Œ MediaCrawlerï¼Œæ— éœ€å‘½ä»¤è¡Œæ“ä½œ")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ åŸºç¡€é…ç½®")

    platform = st.selectbox(
        "é€‰æ‹©å¹³å°",
        options=list(PLATFORMS.keys()),
        format_func=lambda x: f"{x} - {PLATFORMS[x]}",
        help="é€‰æ‹©è¦çˆ¬å–çš„å¹³å°",
    )

    login_type = st.selectbox(
        "ç™»å½•æ–¹å¼",
        options=list(LOGIN_TYPES.keys()),
        format_func=lambda x: LOGIN_TYPES[x],
        help="é€‰æ‹©ç™»å½•æ–¹å¼",
    )

    crawler_type = st.selectbox(
        "çˆ¬è™«ç±»å‹",
        options=list(CRAWLER_TYPES.keys()),
        format_func=lambda x: CRAWLER_TYPES[x],
        help="é€‰æ‹©çˆ¬å–æ¨¡å¼",
    )

    st.divider()
    st.header("ğŸ”§ é€šç”¨è®¾ç½®")

    save_option = st.selectbox(
        "ä¿å­˜æ ¼å¼",
        options=SAVE_OPTIONS,
        index=0,
        help="æ•°æ®ä¿å­˜æ ¼å¼",
    )

    save_path = st.text_input(
        "ä¿å­˜è·¯å¾„ (å¯é€‰)",
        placeholder="é»˜è®¤: MediaCrawler/data/",
        help="è‡ªå®šä¹‰æ•°æ®ä¿å­˜è·¯å¾„ï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤è·¯å¾„",
    )

    max_comments = st.number_input(
        "å•ç¯‡æœ€å¤§è¯„è®ºæ•°",
        min_value=0,
        max_value=10000,
        value=100,
        help="æ¯ç¯‡ç¬”è®°/è§†é¢‘è·å–çš„æœ€å¤§è¯„è®ºæ•°ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶",
    )

    col1, col2 = st.columns(2)
    with col1:
        get_comment = st.checkbox("è·å–è¯„è®º", value=False)
    with col2:
        get_sub_comment = st.checkbox("è·å–å­è¯„è®º", value=False)

    headless = st.checkbox("æ— å¤´æ¨¡å¼", value=True, help="åå°è¿è¡Œæµè§ˆå™¨ï¼ˆä¸æ˜¾ç¤ºçª—å£ï¼‰")

# ä¸»ç•Œé¢ - åŠ¨æ€è¡¨å•
st.header("ğŸ“‹ çˆ¬å–å‚æ•°")

if crawler_type == "search":
    st.subheader("ğŸ” æœç´¢æ¨¡å¼é…ç½®")
    keywords = st.text_area(
        "æœç´¢å…³é”®è¯",
        placeholder="è¾“å…¥å…³é”®è¯ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œå¦‚ï¼šç¾é£Ÿ,æ—…æ¸¸,ç©¿æ­",
        help="è¾“å…¥è¦æœç´¢çš„å…³é”®è¯",
    )
    start_page = st.number_input("èµ·å§‹é¡µç ", min_value=1, value=1, help="ä»ç¬¬å‡ é¡µå¼€å§‹çˆ¬å–")

elif crawler_type == "detail":
    st.subheader("ğŸ“„ è¯¦æƒ…æ¨¡å¼é…ç½®")
    specified_ids = st.text_area(
        "ç¬”è®°/è§†é¢‘ URL æˆ– ID",
        placeholder="è¾“å…¥ URL æˆ– IDï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”",
        help="è¾“å…¥è¦çˆ¬å–çš„ç¬”è®°æˆ–è§†é¢‘é“¾æ¥/ID",
    )
    start_page = st.number_input("èµ·å§‹é¡µç ", min_value=1, value=1)

elif crawler_type == "creator":
    st.subheader("ğŸ‘¤ åˆ›ä½œè€…æ¨¡å¼é…ç½®")
    creator_ids = st.text_area(
        "åˆ›ä½œè€…ä¸»é¡µ URL æˆ– ID",
        placeholder="è¾“å…¥åˆ›ä½œè€…ä¸»é¡µ URL æˆ– IDï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”",
        help="è¾“å…¥åˆ›ä½œè€…ä¸»é¡µé“¾æ¥æˆ–ID",
    )
    start_page = st.number_input("èµ·å§‹é¡µç ", min_value=1, value=1)

# å‚æ•°é¢„è§ˆ
st.divider()
with st.expander("ğŸ“œ å‘½ä»¤é¢„è§ˆ"):
    # æ„å»ºå‚æ•°ç”¨äºæ˜¾ç¤º
    preview_args = build_args(
        platform=platform,
        login_type=login_type,
        crawler_type=crawler_type,
        keywords=locals().get("keywords", ""),
        specified_ids=locals().get("specified_ids", ""),
        creator_ids=locals().get("creator_ids", ""),
        start_page=start_page,
        get_comment=get_comment,
        get_sub_comment=get_sub_comment,
        headless=headless,
        save_option=save_option,
        max_comments=max_comments,
        save_path=save_path,
    )
    cmd_str = f"cd {MEDIA_CRAWLER_PATH} && uv run main.py " + " ".join(preview_args)
    st.code(cmd_str, language="bash")

# è¿è¡ŒæŒ‰é’®
st.divider()
if st.button("ğŸš€ å¼€å§‹çˆ¬å–", type="primary", use_container_width=True):
    # éªŒè¯å¿…å¡«å‚æ•°
    if crawler_type == "search" and not keywords:
        st.error("âŒ æœç´¢æ¨¡å¼éœ€è¦å¡«å†™å…³é”®è¯")
    elif crawler_type == "detail" and not specified_ids:
        st.error("âŒ è¯¦æƒ…æ¨¡å¼éœ€è¦å¡«å†™ç¬”è®°/è§†é¢‘ URL æˆ– ID")
    elif crawler_type == "creator" and not creator_ids:
        st.error("âŒ åˆ›ä½œè€…æ¨¡å¼éœ€è¦å¡«å†™åˆ›ä½œè€… ID")
    else:
        # æ„å»ºå‚æ•°
        run_args = build_args(
            platform=platform,
            login_type=login_type,
            crawler_type=crawler_type,
            keywords=locals().get("keywords", ""),
            specified_ids=locals().get("specified_ids", ""),
            creator_ids=locals().get("creator_ids", ""),
            start_page=start_page,
            get_comment=get_comment,
            get_sub_comment=get_sub_comment,
            headless=headless,
            save_option=save_option,
            max_comments=max_comments,
            save_path=save_path,
        )

        # æ˜¾ç¤ºè¿è¡ŒçŠ¶æ€
        st.info("ğŸ”„ æ­£åœ¨å¯åŠ¨çˆ¬è™«...")

        # åˆ›å»ºè¾“å‡ºåŒºåŸŸ
        output_container = st.container()

        try:
            # è¿è¡Œçˆ¬è™«
            process = run_crawler(run_args)

            # å®æ—¶æ˜¾ç¤ºè¾“å‡º
            stdout_placeholder = output_container.empty()
            stderr_placeholder = output_container.empty()

            stdout_output = []
            stderr_output = []

            # è¯»å–è¾“å‡º
            while True:
                stdout_line = process.stdout.readline()
                stderr_line = process.stderr.readline()

                if stdout_line:
                    stdout_output.append(stdout_line)
                    stdout_placeholder.code("".join(stdout_output[-50:]), language="text")

                if stderr_line:
                    stderr_output.append(stderr_line)
                    stderr_placeholder.error("".join(stderr_output[-20:]))

                if process.poll() is not None and not stdout_line and not stderr_line:
                    break

            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = process.wait()

            if return_code == 0:
                st.success("âœ… çˆ¬å–å®Œæˆï¼")
            else:
                st.error(f"âŒ çˆ¬å–å¤±è´¥ï¼Œè¿”å›ç : {return_code}")

        except Exception as e:
            st.error(f"âŒ è¿è¡Œå‡ºé”™: {str(e)}")

# ä½¿ç”¨è¯´æ˜
with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    ### å¿«é€Ÿå¼€å§‹

    1. **é€‰æ‹©å¹³å°**ï¼šåœ¨ä¾§è¾¹æ é€‰æ‹©è¦çˆ¬å–çš„å¹³å°ï¼ˆå°çº¢ä¹¦ã€æŠ–éŸ³ã€Bç«™ç­‰ï¼‰
    2. **é€‰æ‹©ç™»å½•æ–¹å¼**ï¼š
       - **æ‰«ç ç™»å½•**ï¼šä¼šå¼¹å‡ºäºŒç»´ç ï¼Œç”¨æ‰‹æœºæ‰«ç 
       - **æ‰‹æœºå·ç™»å½•**ï¼šè¾“å…¥æ‰‹æœºå·å’ŒéªŒè¯ç 
       - **Cookie ç™»å½•**ï¼šä½¿ç”¨å·²ä¿å­˜çš„ Cookieï¼ˆéœ€è¦æå‰é…ç½®ï¼‰
    3. **é€‰æ‹©çˆ¬è™«ç±»å‹**ï¼š
       - **æœç´¢æ¨¡å¼**ï¼šæŒ‰å…³é”®è¯æœç´¢å†…å®¹
       - **è¯¦æƒ…æ¨¡å¼**ï¼šçˆ¬å–æŒ‡å®šç¬”è®°/è§†é¢‘çš„è¯¦æƒ…
       - **åˆ›ä½œè€…æ¨¡å¼**ï¼šçˆ¬å–æŒ‡å®šåˆ›ä½œè€…çš„æ‰€æœ‰å†…å®¹
    4. **é…ç½®å‚æ•°**ï¼šæ ¹æ®çˆ¬è™«ç±»å‹å¡«å†™ç›¸åº”çš„å‚æ•°
    5. **ç‚¹å‡»å¼€å§‹**ï¼šç‚¹å‡»"å¼€å§‹çˆ¬å–"æŒ‰é’®è¿è¡Œ

    ### æ³¨æ„äº‹é¡¹

    - é¦–æ¬¡ä½¿ç”¨éœ€è¦å…ˆç™»å½•è·å– Cookie
    - å»ºè®®å¼€å¯æ— å¤´æ¨¡å¼ï¼ˆåå°è¿è¡Œï¼‰
    - çˆ¬å–é¢‘ç‡è¿‡é«˜å¯èƒ½å¯¼è‡´è´¦å·å—é™ï¼Œè¯·åˆç†è®¾ç½®å‚æ•°
    - æ•°æ®é»˜è®¤ä¿å­˜åœ¨ MediaCrawler/data/ ç›®å½•ä¸‹

    ### å¸¸è§é—®é¢˜

    **Q: å¦‚ä½•è·å– Cookieï¼Ÿ**
    A: é¦–æ¬¡ä½¿ç”¨é€‰æ‹©"æ‰«ç ç™»å½•"ï¼Œæ‰«ç æˆåŠŸå Cookie ä¼šè‡ªåŠ¨ä¿å­˜ã€‚

    **Q: çˆ¬å–çš„æ•°æ®åœ¨å“ªé‡Œï¼Ÿ**
    A: é»˜è®¤ä¿å­˜åœ¨ MediaCrawler/data/ ç›®å½•ä¸‹ï¼Œå¯ä»¥åœ¨"ä¿å­˜è·¯å¾„"ä¸­è‡ªå®šä¹‰ã€‚

    **Q: å¯ä»¥åŒæ—¶çˆ¬å–å¤šä¸ªå¹³å°å—ï¼Ÿ**
    A: æ¯æ¬¡åªèƒ½é€‰æ‹©ä¸€ä¸ªå¹³å°ï¼Œéœ€è¦å¤šæ¬¡è¿è¡Œæ¥çˆ¬å–ä¸åŒå¹³å°ã€‚
    """)

# é¡µè„š
st.divider()
st.caption("Powered by MediaCrawler | Streamlit ç•Œé¢ v0.1.0")
