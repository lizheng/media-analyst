"""
æ•°æ®è§£æé¡µé¢

ä¸çˆ¬è™«é¡µé¢è§£è€¦ï¼Œç‹¬ç«‹çš„æ•°æ®è§£æåŠŸèƒ½ï¼š
- é€‰æ‹©æœ¬åœ° JSON æ–‡ä»¶æˆ–ç›®å½•
- è‡ªåŠ¨æ£€æµ‹å¹³å°
- è§£æä¸ºç»Ÿä¸€çš„æ•°æ®æ¨¡å‹
- é¢„è§ˆåŸå§‹æ•°æ®

è·¯å¾„åŸºå‡†ï¼šä»¥ MediaCrawler é¡¹ç›®æ ¹ç›®å½•ä¸ºåŸºå‡†ï¼ˆ../MediaCrawlerï¼‰
"""

import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional

import streamlit as st

from media_analyst.core.models import ParsedData, Platform
from media_analyst.core.parser import (
    detect_platform_from_filename,
    parse_json_file,
    parse_json_files,
    posts_to_dataframe,
    comments_to_dataframe,
)
from media_analyst.ui.persistence import get_media_crawler_path

# MediaCrawler æ ¹ç›®å½•ï¼ˆä½œä¸ºè·¯å¾„åŸºå‡†ï¼‰- ä½¿ç”¨ç»Ÿä¸€é…ç½®
MEDIA_CRAWLER_PATH = get_media_crawler_path()

# =============================================================================
# é¡µé¢é…ç½®
# =============================================================================

def init_page():
    """åˆå§‹åŒ–é¡µé¢çŠ¶æ€"""
    if "parser_parsed_data" not in st.session_state:
        st.session_state.parser_parsed_data: Optional[ParsedData] = None
    if "parser_selected_files" not in st.session_state:
        st.session_state.parser_selected_files: List[str] = []
    if "parser_platform_filter" not in st.session_state:
        st.session_state.parser_platform_filter: Optional[Platform] = None


# =============================================================================
# ä¾§è¾¹æ é…ç½®
# =============================================================================

def find_json_files(directory: Path) -> List[Path]:
    """
    é€’å½’æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰ .json æ–‡ä»¶

    Args:
        directory: è¦æœç´¢çš„ç›®å½•

    Returns:
        æ‰¾åˆ°çš„ JSON æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    if not directory.exists():
        return []

    json_files = []
    try:
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .json æ–‡ä»¶ï¼Œæ’é™¤éšè—æ–‡ä»¶å’Œç›®å½•
        for json_file in directory.rglob("*.json"):
            # è·³è¿‡éšè—æ–‡ä»¶ï¼ˆä»¥ . å¼€å¤´çš„æ–‡ä»¶/ç›®å½•ï¼‰
            if any(part.startswith(".") for part in json_file.parts):
                continue
            json_files.append(json_file)
    except PermissionError:
        st.error(f"æƒé™é”™è¯¯ï¼šæ— æ³•è®¿é—®ç›®å½• {directory}")
        return []

    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    json_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return json_files


def render_sidebar() -> dict:
    """æ¸²æŸ“ä¾§è¾¹æ é…ç½®"""
    with st.sidebar:
        st.header("ğŸ“‚ æ•°æ®æ–‡ä»¶")

        # æ–‡ä»¶é€‰æ‹©æ–¹å¼
        input_method = st.radio(
            "é€‰æ‹©è¾“å…¥æ–¹å¼",
            ["ä¸Šä¼ æ–‡ä»¶", "è¾“å…¥ç›®å½•"],
            help="ä¸Šä¼ æœ¬åœ°æ–‡ä»¶æˆ–è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆé€’å½’æŸ¥æ‰¾ JSON æ–‡ä»¶ï¼‰"
        )

        file_paths: List[str] = []

        if input_method == "ä¸Šä¼ æ–‡ä»¶":
            uploaded_files = st.file_uploader(
                "é€‰æ‹© JSON æ–‡ä»¶",
                type=["json"],
                accept_multiple_files=True,
                help="æ”¯æŒåŒæ—¶ä¸Šä¼ å¤šä¸ªæ–‡ä»¶"
            )
            if uploaded_files:
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
                import tempfile
                import os

                temp_dir = tempfile.mkdtemp()
                for uploaded_file in uploaded_files:
                    temp_path = os.path.join(temp_dir, uploaded_file.name)
                    with open(temp_path, "wb") as f:
                        f.write(uploaded_file.getvalue())
                    file_paths.append(temp_path)
        else:
            # è¾“å…¥ç›®å½•æ–¹å¼
            st.caption(f"ğŸ“ è·¯å¾„åŸºå‡†: {MEDIA_CRAWLER_PATH.resolve()}")

            dir_input = st.text_input(
                "ç›®å½•è·¯å¾„ï¼ˆç›¸å¯¹äº MediaCrawler æ ¹ç›®å½•ï¼‰",
                placeholder="data/douyin/json",
                help="è¾“å…¥ç›®å½•è·¯å¾„ï¼Œå°†é€’å½’æŸ¥æ‰¾è¯¥ç›®å½•ä¸‹æ‰€æœ‰ .json æ–‡ä»¶"
            )

            if dir_input:
                # æ„å»ºç»å¯¹è·¯å¾„ï¼ˆåŸºäº MediaCrawler æ ¹ç›®å½•ï¼‰
                target_dir = MEDIA_CRAWLER_PATH / dir_input.strip()

                if target_dir.exists():
                    if target_dir.is_dir():
                        json_files = find_json_files(target_dir)
                        file_paths = [str(f) for f in json_files]

                        st.success(f"âœ… æ‰¾åˆ° {len(file_paths)} ä¸ª JSON æ–‡ä»¶")

                        # æ˜¾ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆå¯æŠ˜å ï¼‰
                        if file_paths:
                            with st.expander(f"ğŸ“„ æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰20ä¸ªï¼‰", expanded=False):
                                for i, f in enumerate(file_paths[:20], 1):
                                    # æ˜¾ç¤ºç›¸å¯¹è·¯å¾„
                                    rel_path = Path(f).relative_to(MEDIA_CRAWLER_PATH)
                                    st.text(f"{i}. {rel_path}")
                                if len(file_paths) > 20:
                                    st.caption(f"... è¿˜æœ‰ {len(file_paths) - 20} ä¸ªæ–‡ä»¶")
                    else:
                        st.error(f"âŒ {dir_input} ä¸æ˜¯ä¸€ä¸ªç›®å½•")
                else:
                    st.error(f"âŒ ç›®å½•ä¸å­˜åœ¨: {target_dir}")

        return {
            "file_paths": file_paths,
        }


# =============================================================================
# ä¸»ç•Œé¢
# =============================================================================

def render_statistics(parsed_data: ParsedData, raw_stats: dict | None = None):
    """æ¸²æŸ“ç»Ÿè®¡ä¿¡æ¯å¡ç‰‡"""
    st.subheader("ğŸ“Š æ•°æ®ç»Ÿè®¡")

    # å»é‡ç»Ÿè®¡
    dedup_stats = parsed_data.deduplication_stats
    has_duplicates = dedup_stats["total_duplicates"] > 0

    # è·å–åŸå§‹è®°å½•æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ raw_statsï¼‰
    if raw_stats:
        original_count = raw_stats.get("total_records", parsed_data.total_records)
        original_success = raw_stats.get("success_count", parsed_data.success_count)
    else:
        original_count = parsed_data.total_records
        original_success = parsed_data.success_count

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        if has_duplicates:
            st.metric("å¸–å­æ•°", len(parsed_data.posts), delta=f"- {dedup_stats['duplicate_posts']}", delta_color="inverse")
        else:
            st.metric("å¸–å­æ•°", len(parsed_data.posts))
    with col2:
        if has_duplicates:
            st.metric("è¯„è®ºæ•°", len(parsed_data.comments), delta=f"- {dedup_stats['duplicate_comments']}", delta_color="inverse")
        else:
            st.metric("è¯„è®ºæ•°", len(parsed_data.comments))
    with col3:
        total_users = len(set(
            [(p.platform.value, p.user_id) for p in parsed_data.posts if p.user_id] +
            [(c.platform.value, c.user_id) for c in parsed_data.comments if c.user_id]
        ))
        st.metric("ç”¨æˆ·æ•°", total_users)
    with col4:
        # æ˜¾ç¤ºåŸå§‹è®°å½•æ•°ï¼Œé¿å…è¯¯è§£ä¸º"æˆåŠŸç‡"
        if has_duplicates:
            st.metric("åŸå§‹è®°å½•", original_count)
        else:
            st.metric("è§£æè®°å½•", original_success)

    # æ˜¾ç¤ºå»é‡æç¤º
    if has_duplicates:
        st.info(f"ğŸ”„ å·²è‡ªåŠ¨å»é‡ï¼šè¿‡æ»¤äº† {dedup_stats['total_duplicates']} æ¡é‡å¤æ•°æ®ï¼ˆä»¥æœ€æ–°æŠ“å–æ—¶é—´ä¸ºå‡†ï¼‰")


def render_posts_table(parsed_data: ParsedData):
    """æ¸²æŸ“å¸–å­æ•°æ®è¡¨æ ¼"""
    if not parsed_data.posts:
        st.info("ğŸ“­ æ²¡æœ‰è§£æåˆ°å¸–å­æ•°æ®")
        return

    st.subheader(f"ğŸ“ å¸–å­æ•°æ® ({len(parsed_data.posts)} æ¡)")

    # è½¬æ¢ä¸º DataFrame
    try:
        df = posts_to_dataframe(parsed_data.posts)

        # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
        display_columns = [
            "content_id", "platform", "content_type", "title", "nickname",
            "liked_count", "collected_count", "comment_count", "share_count",
            "create_time", "crawl_time", "content_url"
        ]
        available_columns = [c for c in display_columns if c in df.columns]
        df_display = df[available_columns]

        # æ˜¾ç¤ºè¡¨æ ¼
        st.dataframe(
            df_display,
            use_container_width=True,
            hide_index=True,
            column_config={
                "content_id": st.column_config.TextColumn("å†…å®¹ID", width="small"),
                "platform": st.column_config.TextColumn("å¹³å°", width="small"),
                "content_type": st.column_config.TextColumn("ç±»å‹", width="small"),
                "title": st.column_config.TextColumn("æ ‡é¢˜", width="large"),
                "nickname": st.column_config.TextColumn("ä½œè€…", width="medium"),
                "liked_count": st.column_config.NumberColumn("ç‚¹èµ", width="small"),
                "collected_count": st.column_config.NumberColumn("æ”¶è—", width="small"),
                "comment_count": st.column_config.NumberColumn("è¯„è®º", width="small"),
                "share_count": st.column_config.NumberColumn("åˆ†äº«", width="small"),
                "create_time": st.column_config.TextColumn("å‘å¸ƒæ—¶é—´", width="medium"),
                "crawl_time": st.column_config.DatetimeColumn("æŠ“å–æ—¶é—´", width="medium", format="YYYY-MM-DD HH:mm:ss"),
                "content_url": st.column_config.LinkColumn("é“¾æ¥", width="medium"),
            }
        )

    except ImportError:
        st.error("éœ€è¦å®‰è£… pandas: uv add pandas")
    except Exception as e:
        st.error(f"è¡¨æ ¼æ¸²æŸ“å¤±è´¥: {e}")


def render_comments_table(parsed_data: ParsedData):
    """æ¸²æŸ“è¯„è®ºæ•°æ®è¡¨æ ¼"""
    if not parsed_data.comments:
        st.info("ğŸ“­ æ²¡æœ‰è§£æåˆ°è¯„è®ºæ•°æ®")
        return

    st.subheader(f"ğŸ’¬ è¯„è®ºæ•°æ® ({len(parsed_data.comments)} æ¡)")

    try:
        df = comments_to_dataframe(parsed_data.comments)

        display_columns = [
            "comment_id", "content_id", "platform", "content", "nickname",
            "like_count", "sub_comment_count", "create_time", "crawl_time", "is_sub_comment"
        ]
        available_columns = [c for c in display_columns if c in df.columns]
        df_display = df[available_columns]

        st.dataframe(
            df_display,
            use_container_width=True,
            hide_index=True,
            column_config={
                "comment_id": st.column_config.TextColumn("è¯„è®ºID", width="small"),
                "content_id": st.column_config.TextColumn("å†…å®¹ID", width="small"),
                "platform": st.column_config.TextColumn("å¹³å°", width="small"),
                "content": st.column_config.TextColumn("å†…å®¹", width="large"),
                "nickname": st.column_config.TextColumn("ç”¨æˆ·", width="medium"),
                "like_count": st.column_config.NumberColumn("ç‚¹èµ", width="small"),
                "sub_comment_count": st.column_config.NumberColumn("å›å¤", width="small"),
                "create_time": st.column_config.TextColumn("è¯„è®ºæ—¶é—´", width="medium"),
                "crawl_time": st.column_config.DatetimeColumn("æŠ“å–æ—¶é—´", width="medium", format="YYYY-MM-DD HH:mm:ss"),
                "is_sub_comment": st.column_config.CheckboxColumn("å­è¯„è®º", width="small"),
            }
        )

    except ImportError:
        st.error("éœ€è¦å®‰è£… pandas: uv add pandas")
    except Exception as e:
        st.error(f"è¡¨æ ¼æ¸²æŸ“å¤±è´¥: {e}")


def render_raw_preview(file_paths: List[str]):
    """æ¸²æŸ“åŸå§‹æ•°æ®é¢„è§ˆ"""
    with st.expander("ğŸ” åŸå§‹ JSON é¢„è§ˆ", expanded=False):
        if not file_paths:
            st.info("è¯·å…ˆé€‰æ‹©æ–‡ä»¶æˆ–ç›®å½•")
            return

        # æ ¼å¼åŒ–æ˜¾ç¤ºå‡½æ•°ï¼šæ˜¾ç¤ºç›¸å¯¹è·¯å¾„
        def format_path(path_str: str) -> str:
            try:
                path = Path(path_str)
                # å°è¯•æ˜¾ç¤ºç›¸å¯¹äº MediaCrawler çš„è·¯å¾„
                if MEDIA_CRAWLER_PATH in path.parents or path == MEDIA_CRAWLER_PATH:
                    rel_path = path.relative_to(MEDIA_CRAWLER_PATH)
                    return str(rel_path)
                return path.name
            except ValueError:
                return Path(path_str).name

        # é€‰æ‹©è¦é¢„è§ˆçš„æ–‡ä»¶
        preview_file = st.selectbox(
            "é€‰æ‹©æ–‡ä»¶é¢„è§ˆ",
            file_paths,
            format_func=format_path
        )

        if preview_file and Path(preview_file).exists():
            try:
                with open(preview_file, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # æ˜¾ç¤ºå®Œæ•´è·¯å¾„
                full_path = Path(preview_file)
                try:
                    rel_path = full_path.relative_to(MEDIA_CRAWLER_PATH)
                    st.caption(f"ğŸ“ {rel_path}")
                except ValueError:
                    st.caption(f"ğŸ“ {full_path}")

                # é™åˆ¶é¢„è§ˆå¤§å°
                if isinstance(data, list) and len(data) > 5:
                    st.caption(f"å…± {len(data)} æ¡è®°å½•ï¼Œæ˜¾ç¤ºå‰ 5 æ¡")
                    data = data[:5]

                st.json(data)
            except Exception as e:
                st.error(f"æ— æ³•é¢„è§ˆæ–‡ä»¶: {e}")


def render_errors(parsed_data: ParsedData):
    """æ¸²æŸ“é”™è¯¯ä¿¡æ¯"""
    if not parsed_data.errors:
        return

    with st.expander(f"âš ï¸ è§£æé”™è¯¯ ({len(parsed_data.errors)} ä¸ª)", expanded=False):
        for error in parsed_data.errors[:20]:  # æœ€å¤šæ˜¾ç¤º20ä¸ª
            st.warning(error)
        if len(parsed_data.errors) > 20:
            st.caption(f"... è¿˜æœ‰ {len(parsed_data.errors) - 20} ä¸ªé”™è¯¯")


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    """æ•°æ®è§£æé¡µé¢ä¸»å‡½æ•°"""
    st.set_page_config(
        page_title="Media Analyst - æ•°æ®è§£æ",
        page_icon="ğŸ“Š",
        layout="wide"
    )

    init_page()

    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ“Š æ•°æ®è§£æ")
    st.caption("è§£æ MediaCrawler æŠ“å–çš„ JSON æ•°æ®ï¼Œè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼")

    # ä¾§è¾¹æ é…ç½®
    config = render_sidebar()

    # ä¸»ç•Œé¢
    if not config["file_paths"]:
        st.info("ğŸ‘ˆ è¯·åœ¨ä¾§è¾¹æ é€‰æ‹©æˆ–ä¸Šä¼  JSON æ•°æ®æ–‡ä»¶")

        # æ˜¾ç¤ºæ”¯æŒçš„æ ¼å¼è¯´æ˜
        with st.expander("ğŸ“– æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"):
            st.markdown("""
            ### è‡ªåŠ¨æ£€æµ‹å¹³å°
            ç³»ç»Ÿä¼šæ ¹æ® JSON å­—æ®µè‡ªåŠ¨æ£€æµ‹æ•°æ®æ‰€å±å¹³å°ï¼š
            - **æŠ–éŸ³** (`aweme_id`): è§†é¢‘ã€è¯„è®ºæ•°æ®
            - **å°çº¢ä¹¦** (`note_id`): ç¬”è®°ã€è¯„è®ºæ•°æ®
            - **Bç«™** (`bvid`): è§†é¢‘ã€è¯„è®ºæ•°æ®

            ### æ–‡ä»¶å‘½åå»ºè®®
            æ–‡ä»¶åä¸­åŒ…å«å¹³å°åç§°å¯ä»¥å¸®åŠ©æ›´å¿«è¯†åˆ«ï¼š
            - `douyin_contents_2024.json`
            - `xhs_comments_2024.json`
            - `bilibili_data.json`

            ### æ•°æ®æ ¼å¼
            æ”¯æŒä»¥ä¸‹ JSON æ ¼å¼ï¼š
            - å¯¹è±¡åˆ—è¡¨: `[{...}, {...}]`
            - å•ä¸ªå¯¹è±¡: `{...}`
            """)
        return

    # è§£ææŒ‰é’®
    col1, col2 = st.columns([1, 4])
    with col1:
        parse_clicked = st.button("ğŸ” å¼€å§‹è§£æ", type="primary", use_container_width=True)

    # æ‰§è¡Œè§£æ
    if parse_clicked:
        with st.spinner("æ­£åœ¨è§£ææ•°æ®..."):
            try:
                if len(config["file_paths"]) == 1:
                    raw_result = parse_json_file(config["file_paths"][0], deduplicate=False)
                else:
                    raw_result = parse_json_files(config["file_paths"], deduplicate=False)

                # è®¡ç®—å»é‡ç»Ÿè®¡
                dedup_stats = raw_result.deduplication_stats
                has_duplicates = dedup_stats["total_duplicates"] > 0

                # æ‰§è¡Œå»é‡
                if has_duplicates:
                    result = raw_result.deduplicate()
                else:
                    result = raw_result

                st.session_state.parser_parsed_data = result
                st.session_state.parser_raw_stats = {
                    "total_records": raw_result.total_records,
                    "success_count": raw_result.success_count,
                    "duplicate_count": dedup_stats["total_duplicates"],
                    "has_duplicates": has_duplicates,
                }

                # æ„å»ºè¯¦ç»†çš„æˆåŠŸæç¤º
                if has_duplicates:
                    st.success(
                        f"âœ… è§£æå®Œæˆï¼"
                        f"åŸå§‹è®°å½•: {raw_result.total_records} æ¡ | "
                        f"æˆåŠŸè§£æ: {raw_result.success_count} æ¡ | "
                        f"å»é‡å: {len(result.posts) + len(result.comments)} æ¡ "
                        f"(è¿‡æ»¤é‡å¤: {dedup_stats['total_duplicates']} æ¡)"
                    )
                else:
                    st.success(f"âœ… è§£æå®Œæˆï¼æˆåŠŸ {result.success_count}/{result.total_records} æ¡")
            except Exception as e:
                st.error(f"âŒ è§£æå¤±è´¥: {e}")
                return

    # æ˜¾ç¤ºè§£æç»“æœ
    parsed_data = st.session_state.parser_parsed_data
    if parsed_data:
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼ å…¥åŸå§‹ç»Ÿè®¡æ•°æ®ä»¥æ­£ç¡®æ˜¾ç¤ºï¼‰
        raw_stats = st.session_state.get("parser_raw_stats")
        render_statistics(parsed_data, raw_stats)

        st.divider()

        # æ•°æ®è¡¨æ ¼
        tab1, tab2, tab3 = st.tabs(["ğŸ“ å¸–å­", "ğŸ’¬ è¯„è®º", "âš ï¸ é”™è¯¯"])

        with tab1:
            render_posts_table(parsed_data)

        with tab2:
            render_comments_table(parsed_data)

        with tab3:
            render_errors(parsed_data)

        st.divider()

        # åŸå§‹æ•°æ®é¢„è§ˆ
        render_raw_preview(config["file_paths"])


if __name__ == "__main__":
    main()
