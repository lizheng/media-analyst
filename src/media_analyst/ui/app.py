"""
MediaCrawler Streamlit ç•Œé¢

Functional Core, Imperative Shell æ¶æ„ï¼š
1. Core: æ„å»º CrawlerRequest æ¨¡å‹ï¼ˆçº¯å‡½æ•°ï¼‰
2. Shell: CrawlerRunner æ‰§è¡Œï¼ˆå‰¯ä½œç”¨ï¼‰
"""

import platform as _platform_module

import streamlit as st
from pathlib import Path

# ä½¿ç”¨ç»å¯¹å¯¼å…¥ï¼ˆå‡è®¾é€šè¿‡ pip install -e . å®‰è£…ï¼‰
from media_analyst.core import (
    PLATFORMS,
    LOGIN_TYPES,
    CRAWLER_TYPES,
    SAVE_OPTIONS,
    Platform,
    LoginType,
    CrawlerType,
    SaveOption,
    SearchRequest,
    DetailRequest,
    CreatorRequest,
    CrawlerExecution,
    extract_douyin_links,
    format_link_for_display,
)
from media_analyst.core.params import preview_command
from media_analyst.shell import CrawlerRunner, CrawlerRunnerError
from media_analyst.ui.persistence import (
    load_preferences,
    save_from_form_values,
)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="MediaCrawler æ§åˆ¶å°",
    page_icon="ğŸ•·ï¸",
    layout="wide",
)

# MediaCrawler è·¯å¾„
MEDIA_CRAWLER_PATH = Path("../MediaCrawler")


def render_sidebar() -> dict:
    """
    æ¸²æŸ“ä¾§è¾¹æ ï¼Œè¿”å›é€šç”¨é…ç½®

    Returns:
        åŒ…å«é€šç”¨é…ç½®çš„å­—å…¸
    """
    # åŠ è½½ç”¨æˆ·åå¥½ï¼ˆç”¨äºè®¾ç½®é»˜è®¤å€¼ï¼‰
    prefs = load_preferences()

    # è®¡ç®—å„é€‰é¡¹çš„ç´¢å¼•
    platform_options = list(PLATFORMS.keys())
    platform_index = platform_options.index(prefs.platform) if prefs.platform in platform_options else 0

    login_options = list(LOGIN_TYPES.keys())
    login_index = login_options.index(prefs.login_type) if prefs.login_type in login_options else 0

    crawler_options = list(CRAWLER_TYPES.keys())
    crawler_index = crawler_options.index(prefs.crawler_type) if prefs.crawler_type in crawler_options else 0

    save_index = SAVE_OPTIONS.index(prefs.save_option) if prefs.save_option in SAVE_OPTIONS else 0

    with st.sidebar:
        st.header("âš™ï¸ åŸºç¡€é…ç½®")

        platform = st.selectbox(
            "é€‰æ‹©å¹³å°",
            options=platform_options,
            index=platform_index,
            format_func=lambda x: f"{x} - {PLATFORMS[x]}",
            help="é€‰æ‹©è¦çˆ¬å–çš„å¹³å°",
        )

        login_type = st.selectbox(
            "ç™»å½•æ–¹å¼",
            options=login_options,
            index=login_index,
            format_func=lambda x: LOGIN_TYPES[x],
            help="é€‰æ‹©ç™»å½•æ–¹å¼",
        )

        crawler_type = st.selectbox(
            "çˆ¬è™«ç±»å‹",
            options=crawler_options,
            index=crawler_index,
            format_func=lambda x: CRAWLER_TYPES[x],
            help="é€‰æ‹©çˆ¬å–æ¨¡å¼",
        )

        st.divider()
        st.header("ğŸ”§ é€šç”¨è®¾ç½®")

        save_option = st.selectbox(
            "ä¿å­˜æ ¼å¼",
            options=SAVE_OPTIONS,
            index=save_index,
            help="æ•°æ®ä¿å­˜æ ¼å¼",
        )

        save_path = st.text_input(
            "ä¿å­˜è·¯å¾„ (å¯é€‰)",
            value=prefs.save_path,
            placeholder="é»˜è®¤: MediaCrawler/data/",
            help="è‡ªå®šä¹‰æ•°æ®ä¿å­˜è·¯å¾„ï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤è·¯å¾„",
        )

        max_comments = st.number_input(
            "å•ç¯‡æœ€å¤§è¯„è®ºæ•°",
            min_value=0,
            max_value=10000,
            value=prefs.max_comments,
            help="æ¯ç¯‡ç¬”è®°/è§†é¢‘è·å–çš„æœ€å¤§è¯„è®ºæ•°ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶",
        )

        col1, col2 = st.columns(2)
        with col1:
            get_comment = st.checkbox("è·å–è¯„è®º", value=prefs.get_comment)
        with col2:
            get_sub_comment = st.checkbox("è·å–å­è¯„è®º", value=prefs.get_sub_comment)

        headless = st.checkbox("æ— å¤´æ¨¡å¼", value=prefs.headless, help="åå°è¿è¡Œæµè§ˆå™¨ï¼ˆä¸æ˜¾ç¤ºçª—å£ï¼‰")

    return {
        "platform": platform,
        "login_type": login_type,
        "crawler_type": crawler_type,
        "save_option": save_option,
        "save_path": save_path or None,
        "max_comments": max_comments,
        "get_comment": get_comment,
        "get_sub_comment": get_sub_comment,
        "headless": headless,
    }


def render_search_form() -> dict:
    """æ¸²æŸ“æœç´¢æ¨¡å¼è¡¨å•"""
    st.subheader("ğŸ” æœç´¢æ¨¡å¼é…ç½®")
    keywords = st.text_area(
        "æœç´¢å…³é”®è¯",
        placeholder="è¾“å…¥å…³é”®è¯ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œå¦‚ï¼šç¾é£Ÿ,æ—…æ¸¸,ç©¿æ­",
        help="è¾“å…¥è¦æœç´¢çš„å…³é”®è¯",
    )
    start_page = st.number_input("èµ·å§‹é¡µç ", min_value=1, value=1, help="ä»ç¬¬å‡ é¡µå¼€å§‹çˆ¬å–")

    return {
        "keywords": keywords,
        "start_page": start_page,
    }


def render_detail_form(platform: str) -> dict:
    """æ¸²æŸ“è¯¦æƒ…æ¨¡å¼è¡¨å•"""
    st.subheader("ğŸ“„ è¯¦æƒ…æ¨¡å¼é…ç½®")

    # æ ¹æ®å¹³å°æ˜¾ç¤ºä¸åŒçš„æç¤º
    if platform == "dy":
        help_text = (
            "æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼‰ï¼š\n"
            "â€¢ æŠ–éŸ³åˆ†äº«æ–‡æœ¬ï¼ˆè‡ªåŠ¨æå–é“¾æ¥ï¼‰\n"
            "â€¢ çŸ­é“¾ï¼šhttps://v.douyin.com/xxxxx/\n"
            "â€¢ è§†é¢‘é¡µï¼šhttps://www.douyin.com/video/xxxxx\n"
            "â€¢ å›¾æ–‡é¡µï¼šhttps://www.douyin.com/note/xxxxx\n"
            "â€¢ å¤šä¸ªé“¾æ¥ç”¨é€—å·åˆ†éš”"
        )
        placeholder = "ç²˜è´´æŠ–éŸ³åˆ†äº«æ–‡æœ¬æˆ–é“¾æ¥ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”"
    else:
        help_text = "è¾“å…¥è¦çˆ¬å–çš„ç¬”è®°æˆ–è§†é¢‘é“¾æ¥/ID"
        placeholder = "è¾“å…¥ URL æˆ– IDï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”"

    specified_ids = st.text_area(
        "ç¬”è®°/è§†é¢‘ URL æˆ– ID",
        placeholder=placeholder,
        help=help_text,
    )

    # æŠ–éŸ³å¹³å°ï¼šå®æ—¶é¢„è§ˆè§£æç»“æœ
    parsed_links = []
    if platform == "dy" and specified_ids.strip():
        parsed_links = extract_douyin_links(specified_ids)
        if parsed_links:
            with st.expander(f"ğŸ”— å·²è¯†åˆ« {len(parsed_links)} ä¸ªé“¾æ¥", expanded=True):
                for link in parsed_links:
                    st.text(format_link_for_display(link))
                    st.caption(f"æ ‡å‡†åŒ–: {link.normalized}")
        elif specified_ids.strip():
            st.warning("âš ï¸ æœªè¯†åˆ«åˆ°æœ‰æ•ˆçš„æŠ–éŸ³é“¾æ¥")

    start_page = st.number_input("èµ·å§‹é¡µç ", min_value=1, value=1)

    return {
        "specified_ids": specified_ids,
        "start_page": start_page,
        "parsed_links": parsed_links,
    }


def render_creator_form() -> dict:
    """æ¸²æŸ“åˆ›ä½œè€…æ¨¡å¼è¡¨å•"""
    st.subheader("ğŸ‘¤ åˆ›ä½œè€…æ¨¡å¼é…ç½®")
    creator_ids = st.text_area(
        "åˆ›ä½œè€…ä¸»é¡µ URL æˆ– ID",
        placeholder="è¾“å…¥åˆ›ä½œè€…ä¸»é¡µ URL æˆ– IDï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”",
        help="è¾“å…¥åˆ›ä½œè€…ä¸»é¡µé“¾æ¥æˆ–ID",
    )
    start_page = st.number_input("èµ·å§‹é¡µç ", min_value=1, value=1)

    return {
        "creator_ids": creator_ids,
        "start_page": start_page,
    }


def build_request(common_config: dict, mode_config: dict) -> SearchRequest | DetailRequest | CreatorRequest:
    """
    æ„å»ºçˆ¬è™«è¯·æ±‚æ¨¡å‹ï¼ˆCore - çº¯å‡½æ•°ï¼‰

    Args:
        common_config: é€šç”¨é…ç½®
        mode_config: æ¨¡å¼ç‰¹å®šé…ç½®

    Returns:
        å…·ä½“çš„è¯·æ±‚æ¨¡å‹

    Raises:
        ValueError: å¦‚æœé…ç½®æ— æ•ˆ
    """
    # è½¬æ¢æšä¸¾
    platform = Platform(common_config["platform"])
    login_type = LoginType(common_config["login_type"])
    save_option = SaveOption(common_config["save_option"])

    crawler_type = common_config["crawler_type"]

    # é€šç”¨å‚æ•°å­—å…¸
    common = {
        "platform": platform,
        "login_type": login_type,
        "get_comment": common_config["get_comment"],
        "get_sub_comment": common_config["get_sub_comment"],
        "headless": common_config["headless"],
        "save_option": save_option,
        "max_comments": common_config["max_comments"],
        "save_path": common_config["save_path"],
    }

    # æ ¹æ®ç±»å‹æ„å»ºå…·ä½“è¯·æ±‚ï¼ˆä¸åˆæ³•çŠ¶æ€æ— æ³•è¡¨ç¤ºï¼‰
    if crawler_type == "search":
        keywords = mode_config.get("keywords", "").strip()
        if not keywords:
            raise ValueError("æœç´¢æ¨¡å¼å¿…é¡»å¡«å†™å…³é”®è¯")
        return SearchRequest(
            **common,
            keywords=keywords,
            start_page=mode_config.get("start_page", 1),
        )

    elif crawler_type == "detail":
        # å¦‚æœå­˜åœ¨è§£æåçš„é“¾æ¥ï¼ˆæŠ–éŸ³å¹³å°ï¼‰ï¼Œä½¿ç”¨æ ‡å‡†åŒ–åçš„é“¾æ¥
        parsed_links = mode_config.get("parsed_links", [])
        if parsed_links:
            # ä½¿ç”¨æ ‡å‡†åŒ–åçš„é“¾æ¥ï¼Œé€—å·åˆ†éš”
            normalized_urls = [link.normalized for link in parsed_links]
            specified_ids = ",".join(normalized_urls)
        else:
            specified_ids = mode_config.get("specified_ids", "").strip()

        if not specified_ids:
            raise ValueError("è¯¦æƒ…æ¨¡å¼å¿…é¡»å¡«å†™ç¬”è®°/è§†é¢‘ URL æˆ– ID")
        return DetailRequest(
            **common,
            specified_ids=specified_ids,
            start_page=mode_config.get("start_page", 1),
        )

    elif crawler_type == "creator":
        creator_ids = mode_config.get("creator_ids", "").strip()
        if not creator_ids:
            raise ValueError("åˆ›ä½œè€…æ¨¡å¼å¿…é¡»å¡«å†™åˆ›ä½œè€… ID")
        return CreatorRequest(
            **common,
            creator_ids=creator_ids,
            start_page=mode_config.get("start_page", 1),
        )

    else:
        raise ValueError(f"æœªçŸ¥çš„çˆ¬è™«ç±»å‹: {crawler_type}")


def open_results_directory(save_path: str | None) -> None:
    """æ‰“å¼€ç»“æœç›®å½•

    æ³¨æ„ï¼šè·¯å¾„æ˜¯ç›¸å¯¹äº MediaCrawler ç›®å½•çš„ï¼Œå› ä¸ºçˆ¬è™«åœ¨é‚£é‡Œè¿è¡Œ
    """
    import subprocess

    # ç¡®å®šè¦æ‰“å¼€çš„ç›®å½•è·¯å¾„ï¼ˆç›¸å¯¹äº MEDIA_CRAWLER_PATHï¼‰
    if save_path:
        # ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„æ˜¯ç›¸å¯¹äº MediaCrawler çš„
        target_path = MEDIA_CRAWLER_PATH / save_path
    else:
        target_path = MEDIA_CRAWLER_PATH / "data"

    # è§£æä¸ºç»å¯¹è·¯å¾„å¹¶è§„èŒƒåŒ–
    target_path = target_path.resolve()

    if not target_path.exists():
        st.warning(f"ç›®å½•ä¸å­˜åœ¨: {target_path}")
        return

    # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©æ‰“å¼€æ–¹å¼
    system = _platform_module.system()
    try:
        if system == "Darwin":  # macOS
            subprocess.Popen(["open", str(target_path)])
        elif system == "Windows":
            subprocess.Popen(["explorer", str(target_path)])
        else:  # Linux
            subprocess.Popen(["xdg-open", str(target_path)])
        st.toast(f"å·²æ‰“å¼€ç›®å½•: {target_path}")
    except Exception as e:
        st.error(f"æ— æ³•æ‰“å¼€ç›®å½•: {e}")


def run_crawler_ui(request: SearchRequest | DetailRequest | CreatorRequest) -> CrawlerExecution | None:
    """
    è¿è¡Œçˆ¬è™«å¹¶æ˜¾ç¤ºç»“æœï¼ˆShell - å‰¯ä½œç”¨ï¼‰

    Returns:
        CrawlerExecution å¯¹è±¡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
    """
    # åˆå§‹åŒ– Runner
    try:
        runner = CrawlerRunner(MEDIA_CRAWLER_PATH)
    except CrawlerRunnerError as e:
        st.error(f"âŒ {e}")
        return None

    # åˆ›å»ºè¾“å‡ºåŒºåŸŸ
    st.info("ğŸ”„ æ­£åœ¨å¯åŠ¨çˆ¬è™«...")
    output_container = st.container()
    output_placeholder = output_container.empty()

    try:
        # å¯åŠ¨çˆ¬è™«
        execution = runner.start(request)

        # å®æ—¶æ˜¾ç¤ºè¾“å‡ºï¼ˆåˆå¹¶ stdout å’Œ stderrï¼Œä½¿ç”¨æ™®é€šæ–‡æœ¬æ ·å¼ï¼‰
        all_lines = []

        for line in runner.iter_output(execution, timeout=300):  # 5åˆ†é’Ÿè¶…æ—¶
            if line.startswith("[stderr] "):
                all_lines.append(line[9:])
            else:
                all_lines.append(line)
            # åªæ˜¾ç¤ºæœ€å100è¡Œï¼Œä½¿ç”¨æ™®é€šcodeæ ·å¼ï¼ˆéçº¢è‰²ï¼‰
            output_placeholder.code("\n".join(all_lines[-100:]), language="text")

        # æ˜¾ç¤ºç»“æœ
        if execution.status.value == "completed":
            st.success(f"âœ… çˆ¬å–å®Œæˆï¼è€—æ—¶ {execution.duration_seconds:.1f} ç§’")
        else:
            st.error(f"âŒ çˆ¬å–å¤±è´¥: {execution.error_message or 'æœªçŸ¥é”™è¯¯'}")

        return execution

    except TimeoutError:
        st.error("âŒ æ‰§è¡Œè¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰")
        return None
    except Exception as e:
        st.error(f"âŒ è¿è¡Œå‡ºé”™: {str(e)}")
        return None


# ========== ä¸»åº”ç”¨ ==========

def main():
    """ä¸»åº”ç”¨å…¥å£"""
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ•·ï¸ MediaCrawler æ§åˆ¶å°")
    st.markdown("é€šè¿‡ Web ç•Œé¢é…ç½®å’Œè¿è¡Œ MediaCrawlerï¼Œæ— éœ€å‘½ä»¤è¡Œæ“ä½œ")

    # ä½¿ç”¨è¯´æ˜ - æŠ˜å çŠ¶æ€ï¼Œæ”¾åœ¨é¡µé¢é¡¶éƒ¨ä¾¿äºæŸ¥çœ‹
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜", expanded=False):
        st.markdown("""
        ### å¿«é€Ÿå¼€å§‹

        1. **é€‰æ‹©å¹³å°**ï¼šåœ¨ä¾§è¾¹æ é€‰æ‹©è¦çˆ¬å–çš„å¹³å°ï¼ˆå°çº¢ä¹¦ã€æŠ–éŸ³ã€Bç«™ç­‰ï¼‰
        2. **é€‰æ‹©ç™»å½•æ–¹å¼**ï¼š
           - **æ‰«ç ç™»å½•**ï¼šä¼šå¼¹å‡ºäºŒç»´ç ï¼Œç”¨æ‰‹æœºæ‰«ç 
           - **æ‰‹æœºå·ç™»å½•**ï¼šè¾“å…¥æ‰‹æœºå·å’ŒéªŒè¯ç 
           - **Cookie ç™»å½•**ï¼šä½¿ç”¨å·²ä¿å­˜çš„ Cookieï¼ˆéœ€è¦æå‰é…ç½®ï¼‰
        3. **é€‰æ‹©çˆ¬è™«ç±»å‹**ï¼š
           - **æœç´¢æ¨¡å¼**ï¼šæŒ‰å…³é”®è¯æœç´¢å†…å®¹
           - **è¯¦æƒ…æ¨¡å¼**ï¼šçˆ¬å–æŒ‡å®šç¬”è®°/è§†é¢‘çš„è¯¦æƒ…
           - **åˆ›ä½œè€…æ¨¡å¼**ï¼šçˆ¬å–æŒ‡å®šåˆ›ä½œè€…çš„æ‰€æœ‰å†…å®¹
        4. **é…ç½®å‚æ•°**ï¼šæ ¹æ®çˆ¬è™«ç±»å‹å¡«å†™ç›¸åº”çš„å‚æ•°
        5. **ç‚¹å‡»å¼€å§‹**ï¼šç‚¹å‡»"å¼€å§‹çˆ¬å–"æŒ‰é’®è¿è¡Œ

        ### æ³¨æ„äº‹é¡¹

        - é¦–æ¬¡ä½¿ç”¨éœ€è¦å…ˆç™»å½•è·å– Cookie
        - å»ºè®®å¼€å¯æ— å¤´æ¨¡å¼ï¼ˆåå°è¿è¡Œï¼‰
        - çˆ¬å–é¢‘ç‡è¿‡é«˜å¯èƒ½å¯¼è‡´è´¦å·å—é™ï¼Œè¯·åˆç†è®¾ç½®å‚æ•°
        - æ•°æ®é»˜è®¤ä¿å­˜åœ¨ MediaCrawler/data/ ç›®å½•ä¸‹
        """)

    st.divider()

    # åˆå§‹åŒ– session state
    if "is_running" not in st.session_state:
        st.session_state.is_running = False

    # ä¾§è¾¹æ é…ç½®
    common_config = render_sidebar()

    # ä¸»ç•Œé¢ - åŠ¨æ€è¡¨å•
    st.header("ğŸ“‹ çˆ¬å–å‚æ•°")

    crawler_type = common_config["crawler_type"]

    # æ ¹æ®çˆ¬è™«ç±»å‹æ¸²æŸ“ä¸åŒè¡¨å•
    if crawler_type == "search":
        mode_config = render_search_form()
    elif crawler_type == "detail":
        mode_config = render_detail_form(common_config["platform"])
    elif crawler_type == "creator":
        mode_config = render_creator_form()
    else:
        st.error(f"æœªçŸ¥çš„çˆ¬è™«ç±»å‹: {crawler_type}")
        return

    # å‚æ•°é¢„è§ˆ
    st.divider()

    # å°è¯•æ„å»ºè¯·æ±‚ï¼ˆç”¨äºé¢„è§ˆï¼‰
    try:
        request = build_request(common_config, mode_config)
        preview_valid = True
    except ValueError as e:
        request = None
        preview_valid = False
        preview_error = str(e)

    with st.expander("ğŸ“œ å‘½ä»¤é¢„è§ˆ"):
        if preview_valid and request:
            cmd_str = preview_command(request, str(MEDIA_CRAWLER_PATH))
            st.code(cmd_str, language="bash")

            # æ˜¾ç¤ºæ¨¡å‹è¯¦æƒ…
            with st.expander("ğŸ” è¯·æ±‚æ¨¡å‹è¯¦æƒ…"):
                st.json(request.model_dump())
        else:
            st.warning(f"â³ {preview_error}")

    # è¿è¡ŒæŒ‰é’®
    st.divider()

    # ä½¿ç”¨ columns å¸ƒå±€ï¼Œè®©å¼€å§‹æŒ‰é’®å’Œæ‰“å¼€ç›®å½•æŒ‰é’®å¹¶æ’
    col1, col2 = st.columns([3, 1])

    with col1:
        start_button = st.button(
            "ğŸš€ å¼€å§‹çˆ¬å–",
            type="primary",
            use_container_width=True,
            disabled=st.session_state.is_running,
        )

    with col2:
        # æ‰“å¼€ç›®å½•æŒ‰é’®ï¼ˆå§‹ç»ˆå¯ç”¨ï¼‰
        if st.button("ğŸ“‚ æ‰“å¼€ç»“æœç›®å½•", use_container_width=True):
            # è·å–å½“å‰é…ç½®çš„ä¿å­˜è·¯å¾„
            save_path = common_config.get("save_path")
            open_results_directory(save_path)

    if start_button:
        if not preview_valid:
            st.error(f"âŒ é…ç½®æ— æ•ˆ: {preview_error}")
        else:
            # ä¿å­˜ç”¨æˆ·åå¥½è®¾ç½®
            save_from_form_values(
                platform=common_config["platform"],
                login_type=common_config["login_type"],
                crawler_type=common_config["crawler_type"],
                save_option=common_config["save_option"],
                max_comments=common_config["max_comments"],
                get_comment=common_config["get_comment"],
                get_sub_comment=common_config["get_sub_comment"],
                headless=common_config["headless"],
                save_path=common_config.get("save_path", ""),
            )
            # è®¾ç½®è¿è¡ŒçŠ¶æ€å¹¶é‡æ–°è¿è¡Œä»¥ç¦ç”¨æŒ‰é’®
            st.session_state.is_running = True
            st.rerun()

    # å¦‚æœå¤„äºè¿è¡ŒçŠ¶æ€ï¼Œæ‰§è¡Œçˆ¬è™«
    if st.session_state.is_running:
        if preview_valid and request:
            execution = run_crawler_ui(request)

            # è¿è¡Œå®Œæˆåï¼Œæ˜¾ç¤ºæ‰“å¼€ç›®å½•æŒ‰é’®ï¼ˆå¦‚æœæˆåŠŸï¼‰
            if execution and execution.status.value == "completed":
                st.divider()
                result_col1, result_col2 = st.columns([2, 1])

                with result_col1:
                    # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶
                    if execution.output_files:
                        with st.expander("ğŸ“ è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"):
                            for f in execution.output_files:
                                st.text(f)

                with result_col2:
                    # å¿«æ·æ‰“å¼€ç›®å½•æŒ‰é’®
                    save_path = common_config.get("save_path")
                    if st.button("ğŸ“‚ æ‰“å¼€ç»“æœç›®å½•", type="primary", use_container_width=True):
                        open_results_directory(save_path)

        # é‡ç½®è¿è¡ŒçŠ¶æ€
        st.session_state.is_running = False

    # é¡µè„š
    st.divider()
    st.caption("Powered by MediaCrawler | Streamlit ç•Œé¢ v0.2.0")


if __name__ == "__main__":
    main()
