"""
MediaCrawler Streamlit ç•Œé¢

Functional Core, Imperative Shell æ¶æ„ï¼š
1. Core: æ„å»º CrawlerRequest æ¨¡å‹ï¼ˆçº¯å‡½æ•°ï¼‰
2. Shell: CrawlerRunner æ‰§è¡Œï¼ˆå‰¯ä½œç”¨ï¼‰
"""

import streamlit as st
from pathlib import Path

# ä½¿ç”¨ç»å¯¹å¯¼å…¥ï¼ˆå‡è®¾é€šè¿‡ pip install -e . å®‰è£…ï¼‰
from media_analyst.core import (
    PLATFORMS,
    LOGIN_TYPES,
    CRAWLER_TYPES,
    SAVE_OPTIONS,
    Platform,
    LoginType,
    CrawlerType,
    SaveOption,
    SearchRequest,
    DetailRequest,
    CreatorRequest,
    CrawlerExecution,
)
from media_analyst.core.params import preview_command
from media_analyst.shell import CrawlerRunner, CrawlerRunnerError

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="MediaCrawler æ§åˆ¶å°",
    page_icon="ğŸ•·ï¸",
    layout="wide",
)

# MediaCrawler è·¯å¾„
MEDIA_CRAWLER_PATH = Path("../MediaCrawler")


def render_sidebar() -> dict:
    """
    æ¸²æŸ“ä¾§è¾¹æ ï¼Œè¿”å›é€šç”¨é…ç½®

    Returns:
        åŒ…å«é€šç”¨é…ç½®çš„å­—å…¸
    """
    with st.sidebar:
        st.header("âš™ï¸ åŸºç¡€é…ç½®")

        platform = st.selectbox(
            "é€‰æ‹©å¹³å°",
            options=list(PLATFORMS.keys()),
            format_func=lambda x: f"{x} - {PLATFORMS[x]}",
            help="é€‰æ‹©è¦çˆ¬å–çš„å¹³å°",
        )

        login_type = st.selectbox(
            "ç™»å½•æ–¹å¼",
            options=list(LOGIN_TYPES.keys()),
            format_func=lambda x: LOGIN_TYPES[x],
            help="é€‰æ‹©ç™»å½•æ–¹å¼",
        )

        crawler_type = st.selectbox(
            "çˆ¬è™«ç±»å‹",
            options=list(CRAWLER_TYPES.keys()),
            format_func=lambda x: CRAWLER_TYPES[x],
            help="é€‰æ‹©çˆ¬å–æ¨¡å¼",
        )

        st.divider()
        st.header("ğŸ”§ é€šç”¨è®¾ç½®")

        save_option = st.selectbox(
            "ä¿å­˜æ ¼å¼",
            options=SAVE_OPTIONS,
            index=0,
            help="æ•°æ®ä¿å­˜æ ¼å¼",
        )

        save_path = st.text_input(
            "ä¿å­˜è·¯å¾„ (å¯é€‰)",
            placeholder="é»˜è®¤: MediaCrawler/data/",
            help="è‡ªå®šä¹‰æ•°æ®ä¿å­˜è·¯å¾„ï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤è·¯å¾„",
        )

        max_comments = st.number_input(
            "å•ç¯‡æœ€å¤§è¯„è®ºæ•°",
            min_value=0,
            max_value=10000,
            value=100,
            help="æ¯ç¯‡ç¬”è®°/è§†é¢‘è·å–çš„æœ€å¤§è¯„è®ºæ•°ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶",
        )

        col1, col2 = st.columns(2)
        with col1:
            get_comment = st.checkbox("è·å–è¯„è®º", value=False)
        with col2:
            get_sub_comment = st.checkbox("è·å–å­è¯„è®º", value=False)

        headless = st.checkbox("æ— å¤´æ¨¡å¼", value=True, help="åå°è¿è¡Œæµè§ˆå™¨ï¼ˆä¸æ˜¾ç¤ºçª—å£ï¼‰")

    return {
        "platform": platform,
        "login_type": login_type,
        "crawler_type": crawler_type,
        "save_option": save_option,
        "save_path": save_path or None,
        "max_comments": max_comments,
        "get_comment": get_comment,
        "get_sub_comment": get_sub_comment,
        "headless": headless,
    }


def render_search_form() -> dict:
    """æ¸²æŸ“æœç´¢æ¨¡å¼è¡¨å•"""
    st.subheader("ğŸ” æœç´¢æ¨¡å¼é…ç½®")
    keywords = st.text_area(
        "æœç´¢å…³é”®è¯",
        placeholder="è¾“å…¥å…³é”®è¯ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œå¦‚ï¼šç¾é£Ÿ,æ—…æ¸¸,ç©¿æ­",
        help="è¾“å…¥è¦æœç´¢çš„å…³é”®è¯",
    )
    start_page = st.number_input("èµ·å§‹é¡µç ", min_value=1, value=1, help="ä»ç¬¬å‡ é¡µå¼€å§‹çˆ¬å–")

    return {
        "keywords": keywords,
        "start_page": start_page,
    }


def render_detail_form() -> dict:
    """æ¸²æŸ“è¯¦æƒ…æ¨¡å¼è¡¨å•"""
    st.subheader("ğŸ“„ è¯¦æƒ…æ¨¡å¼é…ç½®")
    specified_ids = st.text_area(
        "ç¬”è®°/è§†é¢‘ URL æˆ– ID",
        placeholder="è¾“å…¥ URL æˆ– IDï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”",
        help="è¾“å…¥è¦çˆ¬å–çš„ç¬”è®°æˆ–è§†é¢‘é“¾æ¥/ID",
    )
    start_page = st.number_input("èµ·å§‹é¡µç ", min_value=1, value=1)

    return {
        "specified_ids": specified_ids,
        "start_page": start_page,
    }


def render_creator_form() -> dict:
    """æ¸²æŸ“åˆ›ä½œè€…æ¨¡å¼è¡¨å•"""
    st.subheader("ğŸ‘¤ åˆ›ä½œè€…æ¨¡å¼é…ç½®")
    creator_ids = st.text_area(
        "åˆ›ä½œè€…ä¸»é¡µ URL æˆ– ID",
        placeholder="è¾“å…¥åˆ›ä½œè€…ä¸»é¡µ URL æˆ– IDï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”",
        help="è¾“å…¥åˆ›ä½œè€…ä¸»é¡µé“¾æ¥æˆ–ID",
    )
    start_page = st.number_input("èµ·å§‹é¡µç ", min_value=1, value=1)

    return {
        "creator_ids": creator_ids,
        "start_page": start_page,
    }


def build_request(common_config: dict, mode_config: dict) -> SearchRequest | DetailRequest | CreatorRequest:
    """
    æ„å»ºçˆ¬è™«è¯·æ±‚æ¨¡å‹ï¼ˆCore - çº¯å‡½æ•°ï¼‰

    Args:
        common_config: é€šç”¨é…ç½®
        mode_config: æ¨¡å¼ç‰¹å®šé…ç½®

    Returns:
        å…·ä½“çš„è¯·æ±‚æ¨¡å‹

    Raises:
        ValueError: å¦‚æœé…ç½®æ— æ•ˆ
    """
    # è½¬æ¢æšä¸¾
    platform = Platform(common_config["platform"])
    login_type = LoginType(common_config["login_type"])
    save_option = SaveOption(common_config["save_option"])

    crawler_type = common_config["crawler_type"]

    # é€šç”¨å‚æ•°å­—å…¸
    common = {
        "platform": platform,
        "login_type": login_type,
        "get_comment": common_config["get_comment"],
        "get_sub_comment": common_config["get_sub_comment"],
        "headless": common_config["headless"],
        "save_option": save_option,
        "max_comments": common_config["max_comments"],
        "save_path": common_config["save_path"],
    }

    # æ ¹æ®ç±»å‹æ„å»ºå…·ä½“è¯·æ±‚ï¼ˆä¸åˆæ³•çŠ¶æ€æ— æ³•è¡¨ç¤ºï¼‰
    if crawler_type == "search":
        keywords = mode_config.get("keywords", "").strip()
        if not keywords:
            raise ValueError("æœç´¢æ¨¡å¼å¿…é¡»å¡«å†™å…³é”®è¯")
        return SearchRequest(
            **common,
            keywords=keywords,
            start_page=mode_config.get("start_page", 1),
        )

    elif crawler_type == "detail":
        specified_ids = mode_config.get("specified_ids", "").strip()
        if not specified_ids:
            raise ValueError("è¯¦æƒ…æ¨¡å¼å¿…é¡»å¡«å†™ç¬”è®°/è§†é¢‘ URL æˆ– ID")
        return DetailRequest(
            **common,
            specified_ids=specified_ids,
            start_page=mode_config.get("start_page", 1),
        )

    elif crawler_type == "creator":
        creator_ids = mode_config.get("creator_ids", "").strip()
        if not creator_ids:
            raise ValueError("åˆ›ä½œè€…æ¨¡å¼å¿…é¡»å¡«å†™åˆ›ä½œè€… ID")
        return CreatorRequest(
            **common,
            creator_ids=creator_ids,
            start_page=mode_config.get("start_page", 1),
        )

    else:
        raise ValueError(f"æœªçŸ¥çš„çˆ¬è™«ç±»å‹: {crawler_type}")


def run_crawler_ui(request: SearchRequest | DetailRequest | CreatorRequest) -> None:
    """
    è¿è¡Œçˆ¬è™«å¹¶æ˜¾ç¤ºç»“æœï¼ˆShell - å‰¯ä½œç”¨ï¼‰
    """
    # åˆå§‹åŒ– Runner
    try:
        runner = CrawlerRunner(MEDIA_CRAWLER_PATH)
    except CrawlerRunnerError as e:
        st.error(f"âŒ {e}")
        return

    # åˆ›å»ºè¾“å‡ºåŒºåŸŸ
    st.info("ğŸ”„ æ­£åœ¨å¯åŠ¨çˆ¬è™«...")
    output_container = st.container()
    stdout_placeholder = output_container.empty()
    stderr_placeholder = output_container.empty()

    try:
        # å¯åŠ¨çˆ¬è™«
        execution = runner.start(request)

        # å®æ—¶æ˜¾ç¤ºè¾“å‡º
        stdout_lines = []
        stderr_lines = []

        for line in runner.iter_output(execution, timeout=300):  # 5åˆ†é’Ÿè¶…æ—¶
            if line.startswith("[stderr] "):
                stderr_lines.append(line[9:])
                # åªæ˜¾ç¤ºæœ€å20è¡Œé”™è¯¯
                stderr_placeholder.error("\n".join(stderr_lines[-20:]))
            else:
                stdout_lines.append(line)
                # åªæ˜¾ç¤ºæœ€å50è¡Œè¾“å‡º
                stdout_placeholder.code("\n".join(stdout_lines[-50:]), language="text")

        # æ˜¾ç¤ºç»“æœ
        if execution.status.value == "completed":
            st.success(f"âœ… çˆ¬å–å®Œæˆï¼è€—æ—¶ {execution.duration_seconds:.1f} ç§’")

            # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶
            if execution.output_files:
                with st.expander("ğŸ“ è¾“å‡ºæ–‡ä»¶"):
                    for f in execution.output_files:
                        st.text(f)
        else:
            st.error(f"âŒ çˆ¬å–å¤±è´¥: {execution.error_message or 'æœªçŸ¥é”™è¯¯'}")

    except TimeoutError:
        st.error("âŒ æ‰§è¡Œè¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰")
    except Exception as e:
        st.error(f"âŒ è¿è¡Œå‡ºé”™: {str(e)}")


# ========== ä¸»åº”ç”¨ ==========

def main():
    """ä¸»åº”ç”¨å…¥å£"""
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ•·ï¸ MediaCrawler æ§åˆ¶å°")
    st.markdown("é€šè¿‡ Web ç•Œé¢é…ç½®å’Œè¿è¡Œ MediaCrawlerï¼Œæ— éœ€å‘½ä»¤è¡Œæ“ä½œ")

    # ä¾§è¾¹æ é…ç½®
    common_config = render_sidebar()

    # ä¸»ç•Œé¢ - åŠ¨æ€è¡¨å•
    st.header("ğŸ“‹ çˆ¬å–å‚æ•°")

    crawler_type = common_config["crawler_type"]

    # æ ¹æ®çˆ¬è™«ç±»å‹æ¸²æŸ“ä¸åŒè¡¨å•
    if crawler_type == "search":
        mode_config = render_search_form()
    elif crawler_type == "detail":
        mode_config = render_detail_form()
    elif crawler_type == "creator":
        mode_config = render_creator_form()
    else:
        st.error(f"æœªçŸ¥çš„çˆ¬è™«ç±»å‹: {crawler_type}")
        return

    # å‚æ•°é¢„è§ˆ
    st.divider()

    # å°è¯•æ„å»ºè¯·æ±‚ï¼ˆç”¨äºé¢„è§ˆï¼‰
    try:
        request = build_request(common_config, mode_config)
        preview_valid = True
    except ValueError as e:
        request = None
        preview_valid = False
        preview_error = str(e)

    with st.expander("ğŸ“œ å‘½ä»¤é¢„è§ˆ"):
        if preview_valid and request:
            cmd_str = preview_command(request, str(MEDIA_CRAWLER_PATH))
            st.code(cmd_str, language="bash")

            # æ˜¾ç¤ºæ¨¡å‹è¯¦æƒ…
            with st.expander("ğŸ” è¯·æ±‚æ¨¡å‹è¯¦æƒ…"):
                st.json(request.model_dump())
        else:
            st.warning(f"â³ {preview_error}")

    # è¿è¡ŒæŒ‰é’®
    st.divider()
    if st.button("ğŸš€ å¼€å§‹çˆ¬å–", type="primary", use_container_width=True):
        if not preview_valid:
            st.error(f"âŒ é…ç½®æ— æ•ˆ: {preview_error}")
            return

        # è¿è¡Œçˆ¬è™«ï¼ˆShell - å‰¯ä½œç”¨ï¼‰
        run_crawler_ui(request)

    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### å¿«é€Ÿå¼€å§‹

        1. **é€‰æ‹©å¹³å°**ï¼šåœ¨ä¾§è¾¹æ é€‰æ‹©è¦çˆ¬å–çš„å¹³å°ï¼ˆå°çº¢ä¹¦ã€æŠ–éŸ³ã€Bç«™ç­‰ï¼‰
        2. **é€‰æ‹©ç™»å½•æ–¹å¼**ï¼š
           - **æ‰«ç ç™»å½•**ï¼šä¼šå¼¹å‡ºäºŒç»´ç ï¼Œç”¨æ‰‹æœºæ‰«ç 
           - **æ‰‹æœºå·ç™»å½•**ï¼šè¾“å…¥æ‰‹æœºå·å’ŒéªŒè¯ç 
           - **Cookie ç™»å½•**ï¼šä½¿ç”¨å·²ä¿å­˜çš„ Cookieï¼ˆéœ€è¦æå‰é…ç½®ï¼‰
        3. **é€‰æ‹©çˆ¬è™«ç±»å‹**ï¼š
           - **æœç´¢æ¨¡å¼**ï¼šæŒ‰å…³é”®è¯æœç´¢å†…å®¹
           - **è¯¦æƒ…æ¨¡å¼**ï¼šçˆ¬å–æŒ‡å®šç¬”è®°/è§†é¢‘çš„è¯¦æƒ…
           - **åˆ›ä½œè€…æ¨¡å¼**ï¼šçˆ¬å–æŒ‡å®šåˆ›ä½œè€…çš„æ‰€æœ‰å†…å®¹
        4. **é…ç½®å‚æ•°**ï¼šæ ¹æ®çˆ¬è™«ç±»å‹å¡«å†™ç›¸åº”çš„å‚æ•°
        5. **ç‚¹å‡»å¼€å§‹**ï¼šç‚¹å‡»"å¼€å§‹çˆ¬å–"æŒ‰é’®è¿è¡Œ

        ### æ¶æ„è¯´æ˜

        æœ¬é¡¹ç›®é‡‡ç”¨ **Functional Core, Imperative Shell** æ¶æ„ï¼š

        - **Coreï¼ˆçº¯å‡½æ•°ï¼‰**ï¼šæ•°æ®æ¨¡å‹ï¼ˆPydanticï¼‰å’Œå‚æ•°æ„å»ºé€»è¾‘ï¼Œæ— å‰¯ä½œç”¨
        - **Shellï¼ˆå‰¯ä½œç”¨ï¼‰**ï¼šè¿›ç¨‹ç®¡ç†ã€æ–‡ä»¶ç³»ç»Ÿæ“ä½œ
        - **UIï¼ˆStreamlitï¼‰**ï¼šç•Œé¢æ¸²æŸ“å’Œç”¨æˆ·äº¤äº’

        è¿™ç§åˆ†ç¦»ä½¿ä»£ç æ›´æ˜“æµ‹è¯•ã€æ›´æ˜“ç»´æŠ¤ã€‚

        ### æ³¨æ„äº‹é¡¹

        - é¦–æ¬¡ä½¿ç”¨éœ€è¦å…ˆç™»å½•è·å– Cookie
        - å»ºè®®å¼€å¯æ— å¤´æ¨¡å¼ï¼ˆåå°è¿è¡Œï¼‰
        - çˆ¬å–é¢‘ç‡è¿‡é«˜å¯èƒ½å¯¼è‡´è´¦å·å—é™ï¼Œè¯·åˆç†è®¾ç½®å‚æ•°
        - æ•°æ®é»˜è®¤ä¿å­˜åœ¨ MediaCrawler/data/ ç›®å½•ä¸‹
        """)

    # é¡µè„š
    st.divider()
    st.caption("Powered by MediaCrawler | Streamlit ç•Œé¢ v0.2.0")


if __name__ == "__main__":
    main()
